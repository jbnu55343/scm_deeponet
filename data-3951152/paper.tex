%  LaTeX support: latex@mdpi.com 
%  For support, please attach all files needed for compiling as well as the log file, and specify your operating system, LaTeX version, and LaTeX editor.

%=================================================================
\documentclass[data,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
%\documentclass[preprints,article,submit,pdftex,moreauthors]{Definitions/mdpi} 
% For posting an early version of this manuscript as a preprint, you may use "preprints" as the journal. Changing "submit" to "accept" before posting will remove line numbers.

% Below journals will use APA reference format:
% admsci, aieduc, behavsci, businesses, econometrics, economies, education, ejihpe, famsci, games, humans, ijcs, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth

% Below journals will use Chicago reference format:
% arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci

%--------------------
% Class Options:
%--------------------
%----------
% journal
%----------
% Choose between the following MDPI journals:
% accountaudit, acoustics, actuators, addictions, adhesives, admsci, adolescents, aerobiology, aerospace, agriculture, agriengineering, agrochemicals, agronomy, ai, air, algorithms, allergies, alloys, amh, analytica, analytics, anatomia, anesthres, animals, antibiotics, antibodies, antioxidants, applbiosci, appliedchem, appliedmath, appliedphys, applmech, applmicrobiol, applnano, applsci, aquacj, architecture, arm, arthropoda, arts, asc, asi, astronomy, atmosphere, atoms, audiolres, automation, axioms, bacteria, batteries, bdcc, behavsci, beverages, biochem, bioengineering, biologics, biology, biomass, biomechanics, biomed, biomedicines, biomedinformatics, biomimetics, biomolecules, biophysica, biosensors, biosphere, biotech, birds, blockchains, bloods, blsf, brainsci, breath, buildings, businesses, cancers, carbon, cardiogenetics, catalysts, cells, ceramics, challenges, chemengineering, chemistry, chemosensors, chemproc, children, chips, cimb, civileng, cleantechnol, climate, clinbioenerg, clinpract, clockssleep, cmd, cmtr, coasts, coatings, colloids, colorants, commodities, complications, compounds, computation, computers, condensedmatter, conservation, constrmater, cosmetics, covid, crops, cryo, cryptography, crystals, csmf, ctn, curroncol, cyber, dairy, data, ddc, dentistry, dermato, dermatopathology, designs, devices, diabetology, diagnostics, dietetics, digital, disabilities, diseases, diversity, dna, drones, dynamics, earth, ebj, ecm, ecologies, econometrics, economies, education, eesp, ejihpe, electricity, electrochem, electronicmat, electronics, encyclopedia, endocrines, energies, eng, engproc, ent, entomology, entropy, environments, epidemiologia, epigenomes, esa, est, famsci, fermentation, fibers, fintech, fire, fishes, fluids, foods, forecasting, forensicsci, forests, fossstud, foundations, fractalfract, fuels, future, futureinternet, futureparasites, futurepharmacol, futurephys, futuretransp, galaxies, games, gases, gastroent, gastrointestdisord, gastronomy, gels, genealogy, genes, geographies, geohazards, geomatics, geometry, geosciences, geotechnics, geriatrics, glacies, grasses, greenhealth, gucdd, hardware, hazardousmatters, healthcare, hearts, hemato, hematolrep, heritage, higheredu, highthroughput, histories, horticulturae, hospitals, humanities, humans, hydrobiology, hydrogen, hydrology, hygiene, idr, iic, ijerph, ijfs, ijgi, ijmd, ijms, ijns, ijpb, ijt, ijtm, ijtpp, ime, immuno, informatics, information, infrastructures, inorganics, insects, instruments, inventions, iot, j, jal, jcdd, jcm, jcp, jcs, jcto, jdad, jdb, jeta, jfb, jfmk, jimaging, jintelligence, jlpea, jmahp, jmmp, jmms, jmp, jmse, jne, jnt, jof, joitmc, joma, jop, jor, journalmedia, jox, jpbi, jpm, jrfm, jsan, jtaer, jvd, jzbg, kidney, kidneydial, kinasesphosphatases, knowledge, labmed, laboratories, land, languages, laws, life, lights, limnolrev, lipidology, liquids, literature, livers, logics, logistics, lubricants, lymphatics, machines, macromol, magnetism, magnetochemistry, make, marinedrugs, materials, materproc, mathematics, mca, measurements, medicina, medicines, medsci, membranes, merits, metabolites, metals, meteorology, methane, metrics, metrology, micro, microarrays, microbiolres, microelectronics, micromachines, microorganisms, microplastics, microwave, minerals, mining, mmphys, modelling, molbank, molecules, mps, msf, mti, multimedia, muscles, nanoenergyadv, nanomanufacturing, nanomaterials, ncrna, ndt, network, neuroglia, neurolint, neurosci, nitrogen, notspecified, nursrep, nutraceuticals, nutrients, obesities, oceans, ohbm, onco, oncopathology, optics, oral, organics, organoids, osteology, oxygen, parasites, parasitologia, particles, pathogens, pathophysiology, pediatrrep, pets, pharmaceuticals, pharmaceutics, pharmacoepidemiology, pharmacy, philosophies, photochem, photonics, phycology, physchem, physics, physiologia, plants, plasma, platforms, pollutants, polymers, polysaccharides, populations, poultry, powders, preprints, proceedings, processes, prosthesis, proteomes, psf, psych, psychiatryint, psychoactives, psycholint, publications, purification, quantumrep, quaternary, qubs, radiation, reactions, realestate, receptors, recycling, regeneration, religions, remotesensing, reports, reprodmed, resources, rheumato, risks, robotics, rsee, ruminants, safety, sci, scipharm, sclerosis, seeds, sensors, separations, sexes, signals, sinusitis, siuj, skins, smartcities, sna, societies, socsci, software, soilsystems, solar, solids, spectroscj, sports, standards, stats, std, stresses, surfaces, surgeries, suschem, sustainability, symmetry, synbio, systems, tae, targets, taxonomy, technologies, telecom, test, textiles, thalassrep, therapeutics, thermo, timespace, tomography, tourismhosp, toxics, toxins, transplantology, transportation, traumacare, traumas, tropicalmed, universe, urbansci, uro, vaccines, vehicles, venereology, vetsci, vibration, virtualworlds, viruses, vision, waste, water, wem, wevj, wild, wind, women, world, youth, zoonoticdis

%---------
% article
%---------
% The default type of manuscript is "article", but can be replaced by: 
% abstract, addendum, article, benchmark, book, bookreview, briefcommunication, briefreport, casereport, changes, clinicopathologicalchallenge, comment, commentary, communication, conceptpaper, conferenceproceedings, correction, conferencereport, creative, datadescriptor, discussion, entry, expressionofconcern, extendedabstract, editorial, essay, erratum, fieldguide, hypothesis, interestingimages, letter, meetingreport, monograph, newbookreceived, obituary, opinion, proceedingpaper, projectreport, reply, retraction, review, perspective, protocol, shortnote, studyprotocol, supfile, systematicreview, technicalnote, viewpoint, guidelines, registeredreport, tutorial,  giantsinurology, urologyaroundtheworld
% supfile = supplementary materials

%----------
% submit
%----------
% The class option "submit" will be changed to "accept" by the Editorial Office when the paper is accepted. This will only make changes to the frontpage (e.g., the logo of the journal will get visible), the headings, and the copyright information. Also, line numbering will be removed. Journal info and pagination for accepted papers will also be assigned by the Editorial Office.

%------------------
% moreauthors
%------------------
% If there is only one author the class option oneauthor should be used. Otherwise use the class option moreauthors.

%---------
% pdftex
%---------
% The option pdftex is for use with pdfLaTeX. Remove "pdftex" for (1) compiling with LaTeX & dvi2pdf (if eps figures are used) or for (2) compiling with XeLaTeX.

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2025}
\copyrightyear{2025}
%\externaleditor{Firstname Lastname} % More than 1 editor, please add `` and '' before the last editor name
\datereceived{ } 
\daterevised{ } % Comment out if no revised date
\dateaccepted{ } 
\datepublished{ } 
%\datecorrected{} % For corrected papers: "Corrected: XXX" date in the original paper.
%\dateretracted{} % For retracted papers: "Retracted: XXX" date in the original paper.
\hreflink{https://doi.org/} % If needed use \linebreak
%\doinum{}
%\pdfoutput=1 % Uncommented for upload to arXiv.org
%\CorrStatement{yes}  % For updates
%\longauthorlist{yes} % For many authors that exceed the left citation part

%=================================================================
% Add packages and commands here. The following packages are loaded in our class file: fontenc, inputenc, calc, indentfirst, fancyhdr, graphicx, epstopdf, lastpage, ifthen, float, amsmath, amssymb, lineno, setspace, enumitem, mathpazo, booktabs, titlesec, etoolbox, tabto, xcolor, colortbl, soul, multirow, microtype, tikz, totcount, changepage, attrib, upgreek, array, tabularx, pbox, ragged2e, tocloft, marginnote, marginfix, enotez, amsthm, natbib, hyperref, cleveref, scrextend, url, geometry, newfloat, caption, draftwatermark, seqsplit
% cleveref: load \crefname definitions after \begin{document}

%=================================================================
% Please use the following mathematics environments: Theorem, Lemma, Corollary, Proposition, Characterization, Property, Problem, Example, ExamplesandDefinitions, Hypothesis, Remark, Definition, Notation, Assumption
%% For proofs, please use the proof environment (the amsthm package is loaded by the MDPI class).

%=================================================================
% Full title of the paper (Capitalized)
\Title{Operator Learning with Branch–Trunk Factorization for Macroscopic Short-Term Speed Forecasting}


% MDPI internal command: Title for citation in the left column
\TitleCitation{Operator Learning with Branch–Trunk Factorization for Macroscopic Short-Term Speed Forecasting}

\externalbibliography{yes}

% MDPI internal command: Title for citation in the left column

% Author Orchid ID: enter ID or remove command
\newcommand{\orcidauthorA}{0009-0000-1875-0262} % Add \orcidA{} behind the author's name
%\newcommand{\orcidauthorB}{0000-0000-0000-000X} % Add \orcidB{} behind the author's name

% Authors, for the paper (add full first names)
\Author{Bin Yu $^{1}$, Yong Chen $^{1}$, Dawei Luo $^{2}$\orcidA{} and Joonsoo Bae $^{3,}$*}

%\longauthorlist{yes}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Bin Yu, Yong Chen, Joonsoo Bae and Dawei Luo}

% MDPI internal command: Authors, for citation in the left column, only choose below one of them according to the journal style
% If this is a Chicago style journal 
% (arts, genealogy, histories, humanities, jintelligence, laws, literature, religions, risks, socsci): 
% Lastname, Firstname, Firstname Lastname, and Firstname Lastname.

% If this is a APA style journal 
% (admsci, behavsci, businesses, econometrics, economies, education, ejihpe, games, humans, ijfs, journalmedia, jrfm, languages, psycholint, publications, tourismhosp, youth): 
% Lastname, F., Lastname, F., \& Lastname, F.

% If this is a ACS style journal (Except for the above Chicago and APA journals, all others are in the ACS format): 
% Lastname, F.; Lastname, F.; Lastname, F.
\isAPAStyle{%
       \AuthorCitation{Yu, B., Chen, Y., Bae, J., Luo, D.}
         }{%
        \isChicagoStyle{%
        \AuthorCitation{Bin Yu, Yong Chen, Joonsoo Bae, Dawei Luo.}
        }{
        \AuthorCitation{Yu, B.; Chen, Y.;  Bae, J.;  Luo, D.}
        }
}

% Affiliations / Addresses (Add [1] after \address if there is only one affiliation.)
\address{%
$^{1}$ \quad Changzhou Vocational Institute of Mechatronic Technology; Yubin@czimt.edu.cn\\
$^{2}$ \quad Changzhou College of Information Technology; luodawei@czcit.edu.cn\\
$^{3}$ \quad Jeonbuk National University; jsbae@jbnu.ac.kr}

% Contact information of the corresponding author
\corres{Correspondence: jsbae@jbnu.ac.kr; Tel.:  +82-63-270-2333}

% Current address and/or shared authorship
%\firstnote{Current address: Affiliation.}  
% Current address should not be the same as any items in the Affiliation section.

%\secondnote{These authors contributed equally to this work.}
% The commands \thirdnote{} till \eighthnote{} are available for further notes.

%\simplesumm{} % Simple summary

%\conference{} % An extended version of a conference paper
% Abstract (Do not insert blank lines, i.e. \\) 
\abstract{Logistics operations demand real-time visibility and rapid response, yet minute-level traffic speed forecasting remains challenging due to heterogeneous data sources and frequent distribution shifts. This paper proposes a Deep Operator Network (DeepONet)--based framework that treats traffic prediction as learning a mapping from historical states and boundary conditions to future speed states, enabling robust forecasting under changing scenarios. We project logistics demand onto a road network to generate diverse congestion scenarios and employ a branch--trunk architecture to decouple historical dynamics from exogenous contexts. Experiments on both a controlled simulation dataset and the real-world METR-LA benchmark demonstrate that the proposed method outperforms classical regression and deep learning baselines in cross-scenario generalization. Specifically, the operator learning approach effectively adapts to unseen boundary conditions without retraining, establishing a promising direction for resilient and adaptive logistics forecasting.}

\keyword{logistics forecasting; operator learning; spatio-temporal modeling}



% The fields PACS, MSC, and JEL may be left empty or commented out if not applicable
%\PACS{J0101}
%\MSC{}
%\JEL{}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Diversity
%\LSID{\url{http://}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Applied Sciences
%\featuredapplication{Authors are encouraged to provide a concise description of the specific application or a potential application of the work. This section is not mandatory.}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Data
%\dataset{DOI number or link to the deposited data set if the data set is published separately. If the data set shall be published as a supplement to this paper, this field will be filled by the journal editors. In this case, please submit the data set as a supplement.}
%\datasetlicense{License under which the data set is made available (CC0, CC-BY, CC-BY-SA, CC-BY-NC, etc.)}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal BioTech, Fishes, Neuroimaging and Toxins
%\keycontribution{The breakthroughs or highlights of the manuscript. Authors can write one or two sentences to describe the most important part of the paper.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Encyclopedia
%\encyclopediadef{For entry manuscripts only: please provide a brief overview of the entry title instead of an abstract.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Only for the journal Advances in Respiratory Medicine, Future, Sensors and Smart Cities
%\addhighlights{yes}
%\renewcommand{\addhighlights}{%
%
%\noindent This is an obligatory section in ``Advances in Respiratory Medicine'', ``Future'', ``Sensors'' and ``Smart Cities”, whose goal is to increase the discoverability and readability of the article via search engines and other scholars. Highlights should not be a copy of the abstract, but a simple text allowing the reader to quickly and simplified find out what the article is about and what can be cited from it. Each of these parts should be devoted up to 2~bullet points.\vspace{3pt}\\
%\textbf{What are the main findings?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}\vspace{3pt}
%\textbf{What is the implication of the main finding?}
% \begin{itemize}[labelsep=2.5mm,topsep=-3pt]
% \item First bullet.
% \item Second bullet.
% \end{itemize}
%}
\setlength{\headheight}{24.2pt}
\addtolength{\topmargin}{-12.2pt} % 让版心高度不被头部增高吃掉

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
Short-term speed prediction,which involves forecasting vehicle or traffic speeds over brief future intervals, is a cornerstone technology for modern intelligent transportation systems and the advancement of autonomous vehicles\cite{yang2020short}. With the emergence of new formats such as front warehouses, community retail, local warehouses, and hourly delivery, the integration between logistics and the national economy has deepened, significantly increasing the demand of supply chains for real-time visibility and rapid responsiveness has enhanced significantly. Smart logistics, supported by information technology, control techniques, optimization methods, and artificial intelligence, aims to reduce costs and increase efficiency across the entire supply chain through order allocation, vehicle management, route planning, and signal optimization\cite{yuan2021survey}. To maintain stable operations and quickly recover from disruptions, road networks require predictability. Traffic forecasting\cite{vlahogianni2014}, a core capability, encompasses key quantities such as traffic states, road speeds, and travel times. Accurate prediction of states and speeds provides the foundation for platoon control, route guidance, and signal optimization. Travel time prediction serves as an early indicator for scheduling and coordination. In digital-twin-driven online simulations, forecasting further supports rolling evaluation and scenario selection. For safe and resilient operations, speed prediction also enables risk identification and early warning, allowing interventions in high-risk spatio-temporal segments to reduce accidents delays, ultimately improving punctuality and network reliability.

However, short-term road speed forecasting at the minute level faces multiple challenges in real-world environments. The first challenge lies in the heterogeneity and noise at the data level. Vehicle operation data coexist with multi-source sensor data, where missing values, measurement errors, and irregular sampling are common. The spatial coverage of the sensing network is uneven - dense in central urban areas but sparse in suburban regions, resulting in coverage gaps and biased measurements. The second challenge is the complexity of spatio-temporal coupling. Traffic data simultaneously contain static structures and dynamic evolution, with prominent cross-scale dependencies and nonlinear interactions. Deep learning has advanced spatiotemporal prediction by learning expressive, data-driven representations. Recent graph and sequence models capture spatial diffusion and temporal dependencies, improving traffic flow and speed forecasting\cite{li2018dcrnn}. The third challenge is nonstationarity and distribution shift. Conventional neural prediction models map vectors to vectors and typically require retraining or extensive fine-tuning when exogenous or boundary conditions often change. Demand fluctuations, incidents, weather conditions, as well as modification to road networks and timetables occur frequently. These changes make models trained under previous conditions prone to mismatches in new scenarios, and the costs of maintenance and retraining remain high. Consequently, there is a need for modeling paradigms that can explicitly incorporate boundary changes at the input level while maintaining stable accuracy and reducing maintenance costs when scenarios change.

To address these challenges, this study proposes a short-term road speed forecasting framework that directly integrates logistics data with traffic prediction. To the best of our knowledge, no prior research has systematically mapped supply chain information - such as warehouse and customer locations or dynamic demand volumes - into traffic speed prediction while simultaneously applying operator learning\cite{lu2021deeponet} to achieve cross-scenario transferability. We conduct an initial exploration in this direction by projecting logistics demand and warehouse allocation onto the road network, creating learnable boundary conditions, and then applying an operator-learning approach to map historical sequences and contextual information to next-step speed predictions. This provides a novel perspective for bridging supply chain systems and traffic systems.
At the data level, we develop a unified data and evaluation pipeline that performs alignment, validity checks, anomaly removal, and feature standardization. We then split the data into training, validation, and test sets according to different scenarios, enabling us to evaluate the models' robustness under diverse boundary combinations. At the modeling level, we adopt a branch–trunk architecture. The branch network encodes historical speed sequences of each link to capture short-term dynamics. The trunk network encodes contemporaneous exogenous and boundary states such as inflow, outflow, density, occupancy, waiting time, and travel time that represent congestion intensity and downstream constraints. Multiplicative coupling of the two networks creates a mapping from functions to functions, allowing boundary changes to enter the inference process through input variation. This approach maintains accuracy while reduces the need for retraining when scenarios change.

We constructed six Simulation of Urban MObility (SUMO)\cite{krajzewicz2012sumo} scenarios, labeled S001–S006, based on a five-kilometer urban subnetwork, using a time step of 60 seconds to generate link-level data. These scenarios are driven by the Solomon dataset and differ in random seeds, total trip volumes, and order–warehouse allocation strategies. These variations produce distinct origin–destination(OD) combinations, which characterize the paired relationships between origins and destinations, their intensities, and their temporal distributions. These scenarios also include order quantities, vehicle counts or trip numbers, departure times, and service time windows for each OD pair. Different OD combinations determine the spatial and temporal distributions of inflows and outflows across the road network, which in turn shape congestion patterns and boundary conditions, leading to varying levels of prediction difficulty and transfer challenges. For all scenarios, we extract speed, inflow, outflow, density, occupancy, waiting time, and travel time. Inputs are constructed from twelve-step historical speeds together with six contemporaneous contextual features, while the next-step speed serves as the supervisory signal. After validity checks and anomaly filtering, approximately 1.19 million edge–time samples remain. To evaluate cross-scenario transfer, we use S001–S004 for training and validation and reserve S005–S006 as unseen test sets. Within the visible scenarios, we apply an 80/20 temporal split to ensure leakage-free evaluation that encompasses a variety of boundary conditions.
To quantify the benefits of the proposed approach in modeling nonlinearities and history–context interactions, we systematically compare it with Ridge regression, Multilayer Perceptron (MLP), Long Short-Term Memory (LSTM) networks, and Temporal Convolutional Networks (TCN). We further conduct ablation studies to verify the necessity of trunk-side exogenous variables and perform counterfactual perturbations of these variables to illustrate the model's sensitivity and robustness to congestion transitions. Results demonstrate that the proposed design mitigates feature bias caused by heterogeneous and noisy data, improves adaptability to distribution shifts, and enhances the representation of complex spatio-temporal interactions. Challenges such as missing-data handling, explicit spatial coupling, and uncertainty quantification are discussed in the limitations and reserved for future research.

The contributions of this paper are summarized as follows:

\begin{itemize}[leftmargin=*, itemsep=0.4em] 
  \item This study constructs a unified logistics–traffic dataset by integrating Solomon demand data with SUMO-generated link-level states, producing approximately 1.2 million edge–time samples across six distinct scenarios. This dataset provides a reproducible foundation for cross-scene forecasting research.

  \item We propose a Deep Operator Network–based framework that decouples historical speeds, processed through a branch network, from contemporaneous exogenous and boundary states, processed through a trunk network. This approach enables boundary changes to be incorporated as functional inputs, eliminating the need for frequent retraining.

  \item This paper presents systematic evaluations and diagnostic analyses, demonstrating that the proposed method outperforms both classical regression and deep learning baselines. Additionally, ablation and counterfactual experiments confirm the necessity of exogenous features and reveal robust, interpretable responses to congestion transitions.

  \item This work establishes a reproducible modeling and evaluation pipeline that connects logistics demand with traffic forecasting, providing a foundation for adaptive control, predictive routing, and resilient logistics operations.
\end{itemize}


The remainder of the paper is organized as follows.
Section~2 reviews related work in logistics forecasting, traffic prediction, and operator learning. Section~3 introduces the background and formal problem statement. Section~4 describes the data design, feature construction, and the DeepONet architecture along with training protocols. Section~5 presents the experimental results, diagnostics, and ablation studies, followed by a discussion on deployment implications. Section~6 concludes the paper and outlines limitations and future directions.

\section{Related Work}
Short-term speed prediction is not a monolithic concept. Its definition, particularly the duration of the prediction horizon, is highly dependent on the application context. The field is broadly divided into two categories: macroscopic traffic flow forecasting and microscopic vehicle dynamics prediction. In this work, we focus on the former, which aims to predict aggregated traffic speed, typically the average speed of all vehicles on a specific road segment, over short horizons at the link or corridor level. This task is essential for traffic management, signal control, and route guidance. The prediction horizon in this context generally spans minutes, often ranging from 1 to 30 minutes \cite{yang2023short}. In this work, the time interval for prediction is set to 1 minute. The latter category focuses on individual vehicle trajectories and maneuvers over very short horizons and is crucial for autonomous driving and collision avoidance. The prediction horizons in this context are up to 10 seconds\cite{stockem2023self}.

\subsection{Application of Deep Learning Method in Macroscopic Short-term Speed Forecasting}

Classical macroscopic speed forecasting methods include statistical models such as AutoRegressive Integrated Moving Average (ARIMA)\cite{chatfield2000} and Kalman filters\cite{harvey1990}, which are effective for stationary regimes but limited in handling nonlinearity and dynamic boundaries. Simulation platforms like AnyLogic, FlexSim, and SUMO are widely used for prototyping and assessing operations; however they depend heavily on calibration quality and face scalability challenges\cite{chahal2013simulation,rojas2016flexsim}.
With the liferation of ubiquitous sensing and digital infrastructure, deep learning has become a central paradigm for spatiotemporal prediction. 
Lana et al.\cite{lana2017joint} conducted joint feature selection and parameter tuning for short-term traffic flow forecasting based on heuristically optimized multi-layer neural networks.
With the rapid development of deep learning, various neural network architectures have been proposed for traffic prediction tasks.
Besides Multilayer Perceptron (MLP), Convolutional Neural Networks (CNN), and sequence models such as Long Short-Term Memory (LSTM) and Temporal Convolutional Networks (TCN) \cite{lim2021temporal} are widely applied in time-series prediction tasks. 
Yang et al.\cite{yang2020short} combined CNN and LSTM to predict the traffic speed in one region of Suzhou, illustrating better performance of their hybrid structure. 
Despite accuracy gains, many architectures remain brittle under distribution shift and require costly re–training when exogenous or boundary conditions like inflow or occupancy change.

A persistent challenge in macroscopic speed forecasting is transferability across scenes. Distribution shift, sparse sampling, and sensor noise degrade performance outside the training domain \cite{subbaswamy2021evaluating}. In traffic, models trained in one city or corridor often underperform in another without adaptation \cite{zhang2019transfer}. Domain adaptation techniques and adversarial alignment provide partial remedies but frequently entail substantial retraining and engineering overhead. Similar concerns arise in supply–chain forecasting \cite{carbonneau2008application}. Furthermore, the opacity of deep models complicates deployment in safety–critical logistics operations where auditability is required. These limitations motivate frameworks that can natively accommodate boundary variability and enable transparent analysis. Operator learning\cite{lu2021deeponet}, which maps functions to functions, offers a promising avenue to address these challenges.

\subsection{Operator Learning in Scientific Machine Learning}
Operator learning is supported by universal approximation theorem for operators\cite{lu2021deeponet,li2021fno}. A recurring theme is improved generalization under parametric and boundary changes, a property directly relevant to logistics where exogenous conditions evolve frequently.
Operator learning emerged in scientific machine learning to directly approximate mappings between function spaces when classical vector-to-vector learning is inadequate for tasks such as partial differential equation (PDE) solution operators, fractional operators, or control-to-state maps. Its mathematical footing extends universal approximation results from finite-dimensional functions to operators on compact subsets of Banach spaces. If an operator is continuous on a compact set of admissible inputs, then a suitably parameterized neural operator can approximate it uniformly on that set. This perspective justifies learning function-to-function maps rather than compressing all information into fixed-size vectors.

We consider an operator
\begin{equation}\label{eq:op}
\begin{aligned}
\mathcal{G}\colon &\ V \subset C(K_1) \longrightarrow C(K_2),\\
                  &\ u \longmapsto (\mathcal{G}(u))(y), \quad y\in K_2 ,
\end{aligned}
\end{equation}
where \(u\) may encode source terms, initial and boundary conditions, or control signals. The variable \(y\) denotes an evaluation location, which may include spatial coordinates, time, or other query parameters. Training data are triples \(\bigl(u^{(i)}, y^{(i)}, \mathcal{G}(u^{(i)})(y^{(i)})\bigr)\). To obtain a finite representation of the infinite-dimensional input \(u\), choose sensor points \(\{x_j\}_{j=1}^m \subset K_1\) and form

\begin{equation}
\label{eq:sampling}
\mathbf{u}
\big[u(x_1),u(x_2),\dots,u(x_m)\big]^\top \in \mathbb{R}^m .
\end{equation}

Operator learning parameterizes \(\mathcal{G}\) with two subnetworks and a bilinear fusion. The branch network \(g: \mathbb{R}^m \to \mathbb{R}^p\) encodes the input-function samples \(\mathbf{u}\), and the trunk network \(f: K_2 \to \mathbb{R}^p\) encodes the query \(y\). The prediction is

\begin{equation}
\label{eq:deeponet}
\widehat{\mathcal{G}}(u)(y)
= \langle g(\mathbf{u}),\, f(y) \rangle + b_0
= \sum_{k=1}^{p} g_k(\mathbf{u})\, f_k(y) + b_0 ,
\end{equation}
where \(p\) is the embedding dimension, which is interpretable as the rank of a low-rank expansion. Here, \(g_k\) and \(f_k\) are the \(k\)-th components of the branch and trunk embeddings, and \(b_0\) is an optional bias. This realizes the operator mapping by conditioning on \(u\) through the branch embedding and evaluating at arbitrary \(y\) through the trunk embedding, without requiring explicit convolutions or kernels; it therefore accommodates irregular geometries and unaligned samples. Extra context \(c\), such as material or scenario parameters, can be concatenated to the branch input, \(g([\mathbf{u}; c])\), or to the trunk input, \(f([y; c])\).

\subsection{Physical Interpretation of the Architecture}
In the context of traffic forecasting, the branch and trunk networks play distinct physical roles. The branch network processes the historical speed sequence $\mathbf{s}_{t-L+1:t}(e)$, which represents the system inertia or the short-term momentum of traffic flow. This captures the intrinsic dynamics of the vehicles currently on the road.
The trunk network processes the contemporaneous context $\mathbf{u}_t(e)$, including variables such as density, occupancy, and signal phases. This context represents the boundary conditions and external constraints acting on the flow.
The inner product $\langle g(\cdot), f(\cdot) \rangle$ then models the coupling between the system's inertial state and the external environment. This factorization allows the model to learn how different boundary conditions (trunk) modulate the evolution of traffic dynamics (branch), thereby enabling generalization to new scenarios where the boundary conditions change but the underlying physics of flow remains consistent.

The standard training objective of the operator is empirical risk minimization with mean-squared error:
\begin{equation}
\label{eq:mse}
\mathcal{L}(\theta)
= \frac{1}{N} \sum_{i=1}^{N}
\Big(\widehat{\mathcal{G}}*\theta(u^{(i)})(y^{(i)}) - \mathcal{G}(u^{(i)})(y^{(i)})\Big)^2 ,
\end{equation}
where \(\theta\) collects the parameters of \(g\) and \(f\). When physics constraints are available, one may add a residual term in strong or weak form, for example

\begin{equation}
\label{eq:phys}
\mathcal{L}_{\mathrm{total}}
= \mathcal{L}_{\mathrm{data}}
+ \lambda_{\mathrm{phys}} \,\frac{1}{M}\sum_{r=1}^{M}
\bigl\lvert \mathcal{N}_y\!\big[\widehat{\mathcal{G}}_{\theta}(u^{(i)})(y_r)\big] - q(y_r) \bigr\rvert^{2}.
\end{equation}
where \(\mathcal{N}_y[\cdot] = q\) encodes the governing operator in \(y\) and source \(q\). This couples operator learning with physics-informed regularization.

After training, inference proceeds in two steps. Given a new input function $u^\ast$, evaluate it on the same sensors to obtain $\mathbf{u}^\ast$ and compute the branch embedding $g(\mathbf{u}^\ast) \in \mathbb{R}^p$. For any collection of query locations $y$, compute $f(y) \in \mathbb{R}^p$ and take the inner product:
\begin{equation}
\label{eq:inference}
\widehat{\mathcal{G}}(u^\ast)(y) = \langle g(\mathbf{u}^\ast), f(y) \rangle + b_0 .
\end{equation}
Changing \(u^\ast\) only recomputes the branch output; changing \(y\) only recomputes the trunk output, enabling cross-condition generalization and arbitrary-point evaluation. The trunk naturally accepts spatiotemporal queries by setting \(y=(x,t)\). For multi-output targets, one may append a small linear head from the scalar output to multiple channels, or use separate embeddings per channel.

Practical choices include the sensor count \(m\) in Eq.~\eqref{eq:sampling} where more sensors capture finer details of \(u\) but increase cost, the embedding rank \(p\) in Eq.~\eqref{eq:deeponet} controlls expressive power, standardization or nondimensionalization of inputs, and lightweight MLP or residual blocks for both branch and trunk.
Operator learning has demonstrated strong results across several domains: surrogate modeling for fluid and transport PDEs, fractional and integral operators, stochastic dynamics and filtering, control-to-state and model-predictive-control maps, and multi-physics responses. These successes highlight advantages in cross-condition generalization, handling irregular data, and enabling fast, arbitrary-point evaluations after offline training—properties that are directly useful for real-time macroscopic speed forecasting and decision support.

\subsection{Comparison with Classical, Geometric, and Operator Learning}
While classical and temporal deep learning models such as Ridge regression, Multilayer Perceptron (MLP), and Long Short-Term Memory (LSTM) networks effectively capture temporal correlations in stationary time-series, they fundamentally map fixed-size vectors to vectors. This limitation means they lack the mechanism to explicitly handle changing boundary conditions without retraining, often leading to poor generalization under distribution shifts.
While Graph Neural Networks (GNNs) \cite{ye2023build} explicitly model spatial dependencies via a fixed adjacency matrix, they often struggle when the network topology changes or when defining the graph structure is ambiguous. In contrast, DeepONet learns a continuous operator that maps functional inputs to outputs, making it naturally mesh-independent and adaptable to varying boundary conditions without retraining.
Compared to Fourier Neural Operators (FNO) \cite{li2021fno}, which are highly efficient on uniform grids using Fast Fourier Transforms (FFTs), DeepONet offers greater flexibility for irregular geometries and heterogeneous sensor placements common in traffic networks. Our branch--trunk factorization specifically targets the separation of temporal dynamics from exogenous context, a structure that aligns well with the logistics-traffic coupling problem.

This work addresses link-level speed prediction at 60 s resolution to support traffic control and routing. We construct a framework that projects benchmark demand onto a 5 km urban subnetwork and generates microscopic traffic states, producing controlled yet realistic boundary variability for cross-scene transfer. We develop a branch--trunk factorization that disentangles short-history signals from exogenous and boundary context and demonstrate zero-retraining transfer on held-out scenes. We further provide diagnostic and counterfactual analyses that link accuracy gains to regime-consistent behavior and operational interpretability. To our knowledge, the combination of Solomon-driven demand, SUMO-based microscopic states, and operator learning for link-speed forecasting has not been previously reported.

\section{Background and Problem Formulation}

\subsection{Motivation and Data Infrastructure for Macroscopic Short-term Speed Forecasting}

Short-horizon, link-level speed forecasts are both urgently needed and practically attainable. Public agencies seek to lower system-wide logistics costs via congestion mitigation and network reliability, while enterprises aim to reduce operating costs through improved transport scheduling, warehouse tasking, and production planning. These objectives are enabled by high-frequency data streams from loop detectors, video counters, Global Positioning System (GPS) trajectories, and connected vehicles, together with platform-level integration of demand, inventory, production, and shipment records. This big-data infrastructure aligns public--private needs and supplies the covariates required for minute-scale forecasting in Intelligent Transportation Systems (ITS), supporting proactive signal control, dynamic speed limits, incident detection, reliable travel-time estimation, and predictive routing for freight \cite{vlahogianni2014}. At present, however, production datasets with the necessary spatial coverage, temporal resolution, and metadata are often inaccessible due to privacy and governance constraints, heterogeneous sensing deployments, missingness, and the difficulty of aligning exogenous and boundary conditions at scale. In this context, controlled data generation remains a practical and rigorous path. It enables reproducible experiments, systematic ablations, and conterfactual stress tests under well-specified distribution shifts. Looking ahead, continued advances in sensing, communications, and digital integration make it increasingly likely that such real-world data will be collected and shared in near-real time. Our study therefore develops and evaluates methods in advance of this capability, while using synthesized scenarios to ensure coverage, control and reproducibility.

We consider a 5\,km urban subnetwork, defined as a contiguous district whose total centerline roadway length is approximately 5\,km and that contains multiple signalized intersections and boundary inflow and outflow links. The choice of a 5\,km scale is deliberate. It matches the control horizon of corridor- and district-level operations, such as coordinated signal control and variable speed advisories, where minute-resolution predictions are most actionable. Moreover, it is small enough to support reproducible, microscopic simulation with rich heterogeneity at manageable computational cost. It provides several boundary links so that exogenous inflow and outflow can vary across scenarios, which is essential for evaluating cross-scene transfer.
 Demand and customer attributes are taken from the Solomon benchmark and spatially assigned to network nodes, while traffic states are generated with the SUMO microscopic simulator under multiple scenarios \cite{krajzewicz2012sumo, chowdhury2024calibration}. In this research,signals are aggregated at interval $\Delta=60$\,s. Train, validation and test splits are performed by scenario to support cross-scene evaluation and to reflect distribution shift considerations \cite{quinonero2009dataset}.

Given a directed road network with edge set $\mathcal{E}$, SUMO outputs per-interval measurements for each edge $e\in\mathcal{E}$, including mean speed $v_t(e)$, density, occupancy, counts of vehicles entering and leaving, average waiting time, and travel time \cite{krajzewicz2012sumo,chowdhury2024calibration}. These indicators summarize instantaneous traffic state and congestion intensity on each link.
For each edge $e$ and interval $t$, the goal is to predict the next-interval mean speed $y_{t+1}(e)=v_{t+1}(e)$ from a leakage-safe feature vector that combines short speed histories with contemporaneous exogenous variables:
\begin{align}
\mathbf{u}_t(e)
  &= \big[\text{entered}_t,\ \text{left}_t,\ \text{density}_t,\ \text{occupancy}_t,\ 
      \text{waiting\_time}_t,\ \text{travel\_time}_t \big]^{\top}, \\
\mathbf{s}_{t-L+1:t}(e)
  &= \big[v_{t-L+1}(e),\,\ldots,\,v_{t-1}(e),\,v_t(e)\big]^{\top},
\end{align}
with lag order $L=12$ and $v_{t-k}(e)=v(e,t-k\Delta)$. The current-step speed $v_t(e)$ appears only as the last element of the lagged history when forecasting $t{+}1$—never as a contemporaneous feature for $t{+}1$. Standardization is fit on the training scenarios and applied to validation and test to avoid leakage \cite{makridakis2018statistical}.

\subsection{Formal Problem Statement and Model Overview}
Let $f_\theta$ denote a predictor parameterized by $\theta$. The one-step-ahead task is
\begin{equation}
\hat{y}_{t+1}(e)=f_{\theta}\!\left(\mathbf{u}_t(e),\ \mathbf{s}_{t-L+1:t}(e)\right),
\qquad y_{t+1}(e)=v_{t+1}(e).
\end{equation}
We evaluate classical and neural baselines alongside an operator-learning model:
\begin{itemize}
  \item \textit{Lag-1 persistence:} $\hat{y}_{t+1}(e)=v_t(e)$, a naïve yet informative reference common in short-term time-series forecasting \cite{hyndman2021fpp3}.
    \item \textit{Ridge regression:} a linear baseline with $\ell_2$-regularization, providing shrinkage and robustness to multicollinearity; we follow modern treatments and tuning practices \cite{ISLRv2}.
  \item \textit{Multilayer perceptron (MLP):} a feed-forward nonlinear predictor widely used in various scenarios \cite{UATsurvey2024}.
  \item \textit{Seq2Seq (LSTM):} an encoder--decoder recurrent model where an LSTM encoder summarizes $\mathbf{s}_{t-L+1:t}(e)$ into a context vector and a decoder predicts $\hat{y}_{t+1}(e)$ or multi-step outputs; this family remains a strong baseline for time-series forecasting \cite{sutskever2014seq2seq,lim2020dltsf}.
  \item \textit{Operator learning:} an operator-based predictor that maps function inputs to function outputs by factorizing the mapping into a branch network that encodes temporal history and a trunk network that encodes exogenous and boundary context, coupled multiplicatively by inner-product fusion to evaluate at arbitrary query points; we follow recent neural-operator formulations \cite{kovachki2023neuraloperator}.
\end{itemize}
Architectural details, training protocols, and ablations are provided in the Methodology and Results sections.


\section{Methodology}
% Short methodology summary and process flow figure
As illustrated in Fig~\ref{fig:process_flow}, our methodology follows a three-stage pipeline: (i) data and scenario construction where Solomon demand instances are projected and simulated on a 5 km SUMO subnetwork to produce link-level edge states; (ii) feature engineering and dataset assembly that aligns, filters, and standardizes twelve-step speed histories together with contemporaneous exogenous and boundary covariates; and (iii) model learning and diagnostics using a branch--trunk Deep Operator Network that decouples short-term histories from contextual boundary inputs, followed by systematic cross-scene evaluation, ablations, and counterfactual perturbations.

\begin{figure}[H]
\centering
% Use PDF (vector) version for publication-quality embedding
\includegraphics[width=0.9\textwidth]{figures/processFlow.jpeg}
\caption{Project workflow: from Solomon demand mapping and SUMO simulation, through feature construction and operator-style branch--trunk modeling, to cross-scene evaluation and diagnostics.}
\label{fig:process_flow}
\end{figure}


\subsection{Solomon Dataset as the Demand Prior}
We ground the demand layer in the classical Solomon vehicle routing problem with time windows benchmarks \cite{solomon1987,gunawan2021vehicle}. The suite contains 56 instances with 100 customers, organized into six classes—C1, C2, R1, R2, RC1, RC2—where C/R/RC denote clustered, random, and mixed spatial layouts, and the “1” vs. “2” suffix reflects tighter vs.\ looser time windows, often implying a shorter vs.\ longer planning horizon. Each instance places 100 customers on a \(100\times 100\) grid and follows a common schema: node index \(i\), coordinates \((x_i,y_i)\), demand \(q_i\), ready time \(e_i\), due date \(l_i\), and service duration \(d_i\); the depot is node \(0\). File headers specify the fleet-size limit \(K\) and vehicle capacity \(Q\). These fields map directly to our SUMO pipeline: coordinates are projected to the network coordinate reference system and snapped to the nearest nodes and edges; depot identifiers anchor origins; time windows drive release and service scheduling to produce temporally consistent origin-destination(OD) flows; and demands determine vehicle loading and trip counts. We use Solomon because its controlled spatial patterns and time-window tightness create diverse routing pressures and post-assignment congestion, which is essential for stress-testing forecasting models under heterogeneous boundary conditions.


\subsection{Simulation Environment and Dataset Construction}
We consider an urban subnetwork of approximately $5\,\mathrm{km}$ imported into SUMO, and instantiate six scenarios \texttt{S001}--\texttt{S006} that vary random seeds and trip loads to diversify demand \cite{krajzewicz2012sumo}. Beyond the static network, each scenario is parameterized by logistics demand and supply. Customer requests and depot locations shape OD patterns and temporal loading, which in turn drive the edge states observed during simulation.
We ingest (i) customer planar coordinates $(x,y)$ which are projected to the network coordinate reference system, (ii) demand quantity with units or weight, (iii) requested service time windows $[\underline{t},\overline{t}]$, and (iv) depot or warehouse identifiers and coordinates. Orders are snapped to nearest edges and nodes and grouped into time buckets to form OD flows or discrete trips consistent with their time windows and depot assignments.

Given the OD specification, SUMO produces vehicle- and edge-level traces: (i) per-vehicle routes and traversed edge sequences, and, if needed, per-timestep positions; (ii) per-interval edge aggregates, inclluding speed, entered and left, density, occupancy, waitingTime, traveltime; and (iii) per-vehicle summaries. These outputs connect the logistics side, including who, when, from which depot to which customer, with how much load to the traffic side, including which edges are used, with what speeds and queues, enabling supervised learning on edge dynamics under realistic boundary conditions. Table \ref{tab:data_provenance} summarizes the data sources and their roles in linking logistics demand with traffic states.

\begin{table}[H]
\caption{Data sources and logistics--traffic linkage.\label{tab:data_provenance}}
\begin{adjustwidth}{-\extralength}{0cm}
\begin{tabularx}{\fulllength}{CCC}
\toprule
\textbf{Layer}  &  \textbf{Fields }  &  \textbf{Usage} \\
\midrule
Demand (orders) & \texttt{cust\_id}, $(x,y)$, \texttt{qty}, $[t_{\min},t_{\max}]$, \texttt{depot\_id} &
Build OD flows/trips; snap to network; time-bucket by request; define boundary/context for scenes \\
Supply (depots) & depot coordinates; capacity (if available) &
Define sources/sinks; origin assignment for orders \\
Routes (veh) & \texttt{vehroutes.xml}: edge sequences &
Path reconstruction; edge utilization; optional node traversal via topology \\
Edge aggregates & \texttt{edgedata.xml}: speed, entered, left, density, occupancy, waitingTime, traveltime &
Main supervised features/targets; per-interval edge-level learning \\
Vehicle summaries & \texttt{tripinfo.xml}: departures/arrivals; delays &
Consistency checks; calibration/validation of OD temporal profiles \\
\bottomrule
\end{tabularx}
\end{adjustwidth}
\end{table}

% Representative interval counts per scenario were:
% \[
% \texttt{S001}: 589,\;
% \texttt{S002}: 674,\;
% \texttt{S003}: 1248,\;
% \texttt{S004}: 678,\;
% \texttt{S005}: 788,\;
% \texttt{S006}: 1422.
% \]
% We treat \texttt{S001}--\texttt{S004} as \emph{seen} scenes for training/validation and \texttt{S005}--\texttt{S006} as \emph{held-out} scenes for operator generalization.

From each edgedata, we extract per-edge, per-interval measurements including speed, entered, left, density, occupancy, waitingTime, and traveltime. To rigorously evaluate the contribution of spatial information, we construct two distinct feature sets:

\begin{enumerate}
    \item \textbf{Baseline Dataset (No Spatial):} This configuration focuses on temporal dynamics and local boundary conditions. The input vector $\mathbf{x}_t(e)$ concatenates 12 speed lags ($\texttt{lag1}\ldots\texttt{lag12}$) and 6 contemporaneous covariates (density, occupancy, etc.) of the target edge itself, yielding an 18-dimensional input vector. This serves as the primary dataset for benchmarking temporal sequence models.
    \item \textbf{Spatial Dataset (With Spatial):} To capture network-level dependencies, we augment the baseline features with upstream and downstream context. For each target edge, we identify its immediate predecessor and successor links and append their mean speed and density to the input vector. This increases the input dimensionality to 23, allowing models to explicitly learn from spatial propagation effects.
\end{enumerate}

We form supervised pairs \((\mathbf{x}_t(e), y_{t+1}(e))\) using these feature sets, with the scalar target being the next-step speed $y_{t+1}$. The combined dataset has $23{,}379{,}799$ rows before filtering.
To reduce artifacts, we retain rows satisfying validity checks for \(\texttt{traveltime}>0\), nonnegative counts and finite speeds \cite{krajzewicz2012sumo}. Specifically, the raw simulation output generated approximately 23.3 million edge-time samples. However, due to the sparse nature of traffic in the 5km subnetwork, a significant portion (approx. 95\%) of these samples represented zero-speed or empty-road conditions which provide limited supervisory signal for learning congestion dynamics. To focus the model on active traffic states, we filtered out these zero-value samples, resulting in a final high-quality dataset of approximately 1.19 million samples. This filtering process ensures that the model training is driven by meaningful traffic interactions rather than the dominant background of empty roads.
We exclude the current speed at time $t$ from contemporaneous features to avoid leakage; only lagged speeds are used in inputs. Standardization is fit on training scenarios and applied to validation and test to prevent target or covariate leakage \cite{makridakis2018statistical}.
We split by scenario: \texttt{S001}--\texttt{S004} supply training and validation, an 80/20 temporal split within each seen scene, and \texttt{S005}--\texttt{S006} form the test set. The resulting sizes are
\(\text{train}=953{,}351\), \(\text{val}=119{,}168\), \(\text{test}=119{,}168\).

\subsection{Real-World Dataset: METR-LA}
To validate the generalization capability of our framework beyond simulation, we utilize the METR-LA benchmark dataset\cite{li2018dcrnn}, a widely used reference in traffic forecasting. This dataset collects traffic speed readings from 207 loop detectors on the highways of Los Angeles County, spanning a period of 4 months from March 1, 2012 to June 30, 2012.

Unlike the link-level simulation data, METR-LA provides graph-structured data where sensors are nodes in a network. The adjacency matrix is pre-computed based on the driving distance between sensors, using a Gaussian kernel thresholded to retain only strong connections. The data is aggregated to 5-minute intervals, matching the typical control horizon of ITS applications. We use the standard chronological split of 70\% training, 10\% validation, and 20\% testing. This dataset introduces real-world complexities such as sensor noise, missing values, and non-recurrent congestion events, providing a rigorous testbed for evaluating model robustness in complex, non-linear topologies.

\subsection{Baseline and Comparative Models}

We compare (i) \emph{na\"ive persistence}, defined as $\hat{y}_{t+1}(e)=v_t(e)$ \cite{hyndman2021fpp3};
(ii) \emph{Ridge} regression, an L2-regularized linear model applied to the 18-dimensional input \cite{ISLRv2};
(iii) \emph{MLP} operating on the same 18-dimensional input, a choice supported by modern universal-approximation results \cite{UATsurvey2024};
(iv) \emph{LSTM}, which utilizes the same 12-step window \cite{hochreiter1997lstm};
(v) \emph{TCN}, employing dilated causal convolutions on the same 12-step window \cite{bai2018tcn};
(vi) \emph{Transformer}, which incorporates self-attention mechanisms for time-series forecasting \cite{vaswani2017attention};
(vii) \emph{GNN}, or Graph Neural Network, which explicitly models spatial dependencies via graph convolutions \cite{kipf2016semi}.
Unless noted, all models use identical splits and early stopping on validation $R^2$ \cite{bai2021earlystop}.
All baselines consume the same feature set defined above to ensure parity.

\begin{itemize}

\item \textit{Ridge:}
We fit a linear model on $\mathbf{x}_t(e) = [\mathbf{s}_{t-11:t}(e),\, \mathbf{z}_t(e)] \in \mathbb{R}^{18}$:
\begin{equation}
\min_{\boldsymbol{\beta},\,\beta_0}\ \big\| \mathbf{y} - \beta_0\mathbf{1} - \mathbf{X}\boldsymbol{\beta} \big\|_2^2 \;+\; \alpha \|\boldsymbol{\beta}\|_2^2,
\end{equation}
with features standardized using training statistics and intercept $\beta_0$. The regularization $\alpha$ is selected on a log-grid $\{10^{-6},\ldots,10^{2}\}$. Ridge offers a strong linear baseline with high inference throughput.

\item \textit{MLP:}
Two hidden layers of width 256 with Rectified Linear Unit, or ReLU, activation, dropout $0.1$, Adaptive Moment Estimation, known as Adam, optimizer with a learning rate of $10^{-3}$, batch size $8192$, up to 30 epochs; early stopping on validation.

\item \textit{LSTM:}
We form a sequence $\{\mathbf{x}^{(k)}\}_{k=1}^{12}$ where each step uses the $k$-th speed lag and the same exogenous context:
\begin{equation}
\mathbf{x}^{(k)} = [\,\texttt{lag}_k,\; \mathbf{z}_t(e)\,] \in \mathbb{R}^{1+6},
\end{equation}
yielding an input tensor $(\text{batch},\,\text{time}{=}12,\,\text{feat}{=}7)$.
A single-layer LSTM (hidden size 128, dropout 0.1) processes the sequence; the last hidden state feeds a linear head to predict $y_{t+1}$. Optimizer: Adam with a learning rate of $10^{-3}$, batch size $8192$, 30 epochs, and early stopping.

\item \textit{TCN:}
We use a causal Temporal Convolutional Network on the same $(12\times 7)$ sequence: four residual blocks with dilations $[1,2,4,8]$, kernel size $3$, 64 channels, dropout $0.1$; causal padding prevents leakage. The receptive field, which is greater than 12, covers the window. The block output is global-pooled and passed to a linear head. Optimizer and early stopping are applied as described above.

\item \textit{Transformer:}
We employ a standard Transformer encoder architecture adapted for time-series forecasting. The model consists of 2 encoder layers with 4 attention heads, a model dimension of 64, and a feed-forward dimension of 256. Positional encodings are added to the input sequence to retain temporal order information.

\item \textit{GNN:}
We utilize a Graph Convolutional Network, abbreviated as GCN, to capture spatial dependencies. For the simulation dataset, the graph is constructed based on physical connectivity, specifically upstream and downstream links. For the METR-LA dataset, we use the predefined sensor adjacency matrix. The model consists of two GCN layers with 64 hidden units followed by a fully connected output layer.

\end{itemize}

To ensure a fair comparison, we performed a grid search for the hyperparameters of each model using the validation set. The search space included learning rates in $\{10^{-2}, 10^{-3}, 10^{-4}\}$, batch sizes in $\{64, 128, 256, 1024\}$, and dropout rates in $\{0.1, 0.3, 0.5\}$. The final hyperparameters selected for the reported experiments are summarized in Table~\ref{tab:hparams}.

\begin{table}[H]
\caption{Final training hyperparameters used in the study.}
\label{tab:hparams}
\begin{adjustwidth}{-\extralength}{0cm}
\centering
\setlength{\tabcolsep}{4pt}
\footnotesize
\begin{tabular}{lcccccc}
\toprule
Model & Input shape & Regularization & Optimizer \& LR & Batch & Max epochs / ES \\
\midrule
Persistence (lag1) & $18$ (uses \texttt{lag1} only) & --- & --- & --- & --- \\
Ridge              & $18$                           & L2 ($\alpha$ tuned) & closed-form / LBFGS & N/A  & N/A \\
MLP                & $23$                           & Dropout $0.1$ & Adam, $10^{-3}$ & 1024 & 50 / patience 10 \\
LSTM               & $(12 \times 7)$                & Dropout $0.1$ & Adam, $10^{-3}$ & 1024 & 50 / patience 10 \\
TCN                & $(12 \times 7)$                & Dropout $0.1$ & Adam, $10^{-3}$ & 1024 & 50 / patience 10 \\
Transformer        & $(12 \times 7)$                & Dropout $0.1$ & Adam, $10^{-3}$ & 128  & 100 / patience 10 \\
GNN                & Graph ($N \times F$)           & Dropout $0.3$ & Adam, $10^{-3}$ & 64   & 100 / patience 10 \\
DeepONet           & Branch: $12$; Trunk: $6$       & Dropout $0.1$ & Adam, $10^{-3}$ & 1024 & 50 / patience 10 \\
\bottomrule
\end{tabular}
\end{adjustwidth}
\end{table}


\subsection{Operator-Learning Model}

We model the one-step map from an edge’s recent speed history and its contemporaneous context to the next-step speed as a neural operator acting on two inputs: the 12-step lag vector $\mathbf{s}_{t-11:t}(e)\in\mathbb{R}^{12}$ and the 6-d context $\mathbf{z}_t(e)\in\mathbb{R}^{6}$. Let $b:\mathbb{R}^{12}\!\to\!\mathbb{R}^{p}$ and $\tau:\mathbb{R}^{6}\!\to\!\mathbb{R}^{p}$ be branch and trunk embeddings. The prediction is their inner product in a $p$-dimensional latent space:
\begin{equation}
\label{eq:op_pred}
\hat{y}_{t+1}(e)\;=\;\big\langle b\!\big(\mathbf{s}_{t-11:t}(e)\big),\;\tau\!\big(\mathbf{z}_t(e)\big)\big\rangle
\;=\;\sum_{k=1}^{p} b_k\!\big(\mathbf{s}_{t-11:t}(e)\big)\,\tau_k\!\big(\mathbf{z}_t(e)\big),
\end{equation}
which realizes a low-rank factorization of the operator from $(\mathbf{s},\mathbf{z})$ to $y$ \cite{kovachki2023neuraloperator,lu2021deeponet}. The overall architecture is illustrated in Figure~\ref{fig:deeponet_arch}. Architecturally, both branch and trunk are MLPs with hidden width 256, dropout $0.1$, and linear $p$-dimensional projections; we set $p\!=\!128$. Optimization uses Adam with learning rate $10^{-3}$, batch size $1024$, up to 50 epochs with early stopping on validation $R^2$. All features are standardized using training statistics, and train/validation/test splits, random seeds, and library versions are fixed for reproducibility.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig_deeponet_arch.png}
\caption{Schematic of the DeepONet architecture for traffic forecasting. The Branch network encodes the historical speed sequence (system inertia), while the Trunk network encodes the contemporaneous context (boundary conditions). The final prediction is obtained via the dot product of their respective embeddings, effectively learning the operator that maps history and context to future states.}
\label{fig:deeponet_arch}
\end{figure}

The factorized form \eqref{eq:op_pred} decouples temporal history from exogenous conditions and enables counterfactual analyses without retraining: varying $\mathbf{z}_t(e)$, for instance by perturbing \textit{entered} or \textit{density}, changes $\tau(\cdot)$ while keeping $b(\cdot)$ fixed, thus isolating the effect of boundary and context signals on $\hat{y}_{t+1}(e)$.
This branch--trunk inner-product realization exactly matches the DeepONet formulation for operator learning \cite{lu2021deeponet}, so we henceforth refer to our model as DeepONet and use “DeepONet” to denote it throughout.

The theoretical advantage of this operator learning formulation lies in its alignment with the physical nature of traffic flow. Traffic dynamics are fundamentally governed by partial differential equations, where the system state evolves as a function of time and space subject to boundary conditions. Standard deep learning models approximate a finite-dimensional mapping $\mathbb{R}^n \to \mathbb{R}^m$, effectively memorizing point-to-point correlations. In contrast, DeepONet approximates the continuous solution operator that maps the space of input functions and parameter functions to the solution space. By explicitly separating the encoding of history and context, the model learns a basis expansion of the solution operator, where the Trunk network identifies the basis functions of the traffic regimes and the Branch network computes the coefficients based on the input state. This mechanism enables robust generalization to unseen scenarios, as the model learns the underlying physical laws governing the transition between states rather than just the statistical distribution of the training data.

\subsection{Evaluation}
We report Mean Absolute Error (MAE), Root Mean Squared Error (RMSE), and $R^2$:
\begin{equation}
\mathrm{MAE}=\tfrac{1}{N}\sum_i|y_i-\hat{y}_i|,\quad
\mathrm{RMSE}=\sqrt{\tfrac{1}{N}\sum_i(y_i-\hat{y}_i)^2},\quad
R^2=1-\frac{\sum_i(y_i-\hat{y}_i)^2}{\sum_i(y_i-\bar{y})^2}.
\end{equation}
In words, MAE reports the average absolute deviation in $\mathrm{km/h}$ and is relatively robust to outliers; RMSE reports the quadratic mean error and emphasizes large deviations, which is desirable when large mistakes are particularly costly; and $R^2$ reports the proportion of variance explained relative to a mean-only baseline and can be negative if the model underperforms that baseline. Reporting de-standardized MAE and RMSE in $\mathrm{km/h}$ enables operational interpretation, while $R^2$ facilitates scale-free comparison across scenes.
All metrics are computed on de-standardized speeds (km/h) \cite{makridakis2018statistical}.


% ===========================
%  RESULTS (MDPI style)
% ===========================
\section{Experimental Results and Discussion}\label{sec:results}

\subsection{Overall Performance Comparison}
Table~\ref{tab:overall_comparison} presents a comprehensive comparison of model performance across three experimental modules: (1) The SUMO Baseline (temporal features only), (2) The SUMO Spatial module (enhanced with upstream/downstream features), and (3) The Real-World Validation using the METR-LA dataset.

In general, we observe distinct performance patterns across the three scenarios. In the linear simulation (Modules 1 and 2), temporal sequence models like LSTM and DeepONet dominate, as the system dynamics are primarily driven by local history and boundary conditions. Conversely, in the complex METR-LA network (Module 3), the advantage shifts towards architectures capable of modeling high-dimensional spatial interactions, where DeepONet and Transformer achieve state-of-the-art results. Notably, GNNs show a significant performance jump from simulation to real-world, validating their dependency on rich graph structures. While DeepONet is competitive in the simpler simulation tasks, its true strength lies in its robustness and scalability to complex, real-world topologies, where it outperforms traditional baselines by a wide margin.

\begin{table}[H]
\caption{Comparative Performance of Deep Learning Models across Experimental Modules. Best results in \textbf{bold}.}
\label{tab:overall_comparison}
\begin{adjustwidth}{-\extralength}{0cm}
\centering
\setlength{\tabcolsep}{0pt}
\begin{tabular*}{\fulllength}{@{\extracolsep{\fill}}llcccccc}
\toprule
\textbf{Module} & \textbf{Dataset} & \textbf{Model} & \textbf{$R^2$ Score} & \textbf{MAE} & \textbf{RMSE} & \textbf{Train Time (s)} & \textbf{Inf Time (s)} \\
\midrule
\textbf{1. SUMO Baseline} & Simulation (Linear) & Persistence & -1.0749 & 9.66 & 13.75 & 0.0 & 0.0 \\
(No Spatial) & 19 Features & Ridge & 0.4631 & 5.63 & 6.99 & \textbf{0.1} & - \\
& ~1.2M Samples & MLP & 0.7975 & 2.86 & 4.30 & 178.4 & - \\
& & TCN & 0.7905 & 2.97 & 4.37 & 366.2 & - \\
& & LSTM & \textbf{0.8188} & \textbf{2.59} & \textbf{4.06} & 603.2 & - \\
& & Transformer & 0.8152 & 2.61 & 4.10 & 2174.5 & - \\
& & DeepONet & 0.8122 & 2.66 & 4.14 & 362.1 & - \\
\midrule
\textbf{2. SUMO Spatial} & Simulation (Linear) & Persistence & -2.8173 & 11.52 & 15.26 & 0.0 & 0.0 \\
(With Spatial) & 23 Features & Ridge & 0.3224 & 5.23 & 6.43 & \textbf{0.1} & - \\
& ~1.2M Samples & MLP & 0.7031 & 2.97 & 4.26 & 272.0 & - \\
& & TCN & 0.6974 & 2.99 & 4.30 & 378.1 & - \\
& & LSTM & \textbf{0.7483} & \textbf{2.60} & \textbf{3.92} & 522.2 & - \\
& & GNN (Local) & 0.7166 & 2.81 & 4.16 & 428.0 & - \\
& & Transformer & 0.7310 & 2.70 & 4.05 & 2958.4 & - \\
& & DeepONet & 0.7473 & 2.66 & 3.93 & 396.6 & - \\
\midrule
\textbf{3. Real-World} & METR-LA (Graph) & Persistence & 0.3590 & 7.61 & 17.68 & 0.00 & 0.00 \\
(Complex) & 207 Nodes & Ridge & 0.9044 & 3.50 & 6.83 & 2.11 & 0.03 \\
& ~34k Samples & MLP & 0.8791 & 4.23 & 7.68 & 4.50 & 0.14 \\
& & TCN & 0.8949 & 4.10 & 7.16 & 123.74 & 0.39 \\
& & LSTM & 0.7704 & 6.79 & 10.58 & 5.87 & 0.08 \\
& & GNN (GCN) & 0.8952 & 4.56 & 7.15 & 96.00 & 0.18 \\
& & Transformer & 0.9137 & 2.74 & 6.49 & \textbf{73.00} & 0.33 \\
& & \textbf{DeepONet} & \textbf{0.9172} & \textbf{2.55} & \textbf{6.35} & 92.00 & 0.07 \\
\bottomrule
\end{tabular*}
\end{adjustwidth}
\end{table}

\subsection{Baseline Simulation Experiments}\label{sec:module1}
\subsubsection{Implementation details}
To establish a performance benchmark, we first evaluated the models on the standardized SUMO dataset without explicit spatial topology features. The input vector $X_t$ consisted of 19 dimensions, capturing the local temporal history from Lags 1 to 12 and instantaneous traffic variables such as density and occupancy of the target edge.

As shown in Module 1 of Table~\ref{tab:overall_comparison}, the LSTM model achieved the highest accuracy with an $R^2$ score of 0.8188, slightly outperforming the Transformer and DeepONet, which achieved $R^2$ scores of 0.8152 and 0.8122, respectively. The MLP and TCN models followed with $R^2$ scores of 0.7975 and 0.7905. The linear Ridge baseline lagged significantly behind with an $R^2$ of 0.4631, confirming the non-linear nature of the traffic dynamics. These results indicate that for a single road segment in a controlled simulation environment, the temporal autocorrelation is the dominant predictive factor. The strong performance of LSTM, Transformer and DeepONet suggests that capturing sequence dependencies and operator-level mappings provides an advantage even in this baseline setting. Furthermore, DeepONet's performance is comparable to the specialized LSTM, demonstrating that the branch-trunk architecture effectively encodes the temporal inertia through the branch network without requiring recurrent computation.

\subsection{Spatial Feature Analysis}\label{sec:module2}
Addressing the concern regarding the omission of spatial correlations, we extended the feature space to include upstream and downstream dependencies. We constructed a "Spatial" dataset, referred to as Module 2, where the input dimension was increased to 23 by appending the mean speed and density of adjacent links, comprising $v_{up}, v_{down}, k_{up}, k_{down}$.

Counter-intuitively, the inclusion of these local spatial features did not improve performance in the simulation environment; in fact, we observed a slight decrease in $R^2$ across all models, where DeepONet and MLP scored 0.7473 and 0.7031, respectively. We attribute this to two factors:
\begin{enumerate}
    \item \textit{Topology Simplicity:} The simulation utilizes a linear 5km corridor where upstream conditions are highly collinear with the local temporal history; for instance, $v_{up}(t)$ provides similar information to $v_{local}(t-1)$.
    \item \textit{Noise Introduction:} In the microscopic simulation, short-term fluctuations in adjacent links (due to individual driver behavior) may introduce stochastic noise that outweighs their predictive signal for the aggregated 5-minute interval.
\end{enumerate}

This result supports a critical physical interpretation: in the DeepONet framework, the boundary conditions (flow entering/leaving) serve as the interface for wave propagation. In 1D traffic flow (LWR model), congestion waves propagate through the boundaries. By learning the operator that maps these boundary functions to the internal state, DeepONet implicitly learns the wave propagation physics. The fact that explicit spatial features did not improve performance suggests that for this linear topology, the temporal dynamics and boundary conditions were indeed sufficient to capture these effects.

However, this negative result is scientifically valuable: it demonstrates that DeepONet's operator learning capability is robust enough to extract maximum information from temporal dynamics alone, without relying on explicit spatial feature engineering in simple topologies. In this module, LSTM achieved the highest performance with an $R^2$ of 0.7483, closely followed by DeepONet with 0.7473, both outperforming the Transformer ($R^2$ of 0.7310) and GNN ($R^2$ of 0.7166). This reinforces the finding that sequence modeling and operator mapping are more effective than graph-based methods for this specific linear topology. The lower performance of GNN here, with an $R^2$ of approximately 0.72, highlights a limitation of graph convolutions in sparse, linear structures where message passing offers little advantage over direct temporal modeling.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig2_error_distribution.png}
\caption{Robustness analysis of DeepONet vs. baselines. (a) Cross-Scenario Generalization: DeepONet and Transformer maintain stable low error (MAE) when transferring from seen training scenarios (S001-S004) to unseen test scenarios (S005-S006), whereas MLP and GNN performance degrades. (b) Error vs. Traffic Density: As traffic density increases (x-axis), the error of the MLP model grows quadratically, indicating failure in congestion regimes. GNN shows moderate degradation, while DeepONet and Transformer remain robust, validating the effectiveness of operator learning and attention mechanisms in handling varying boundary conditions.}
\label{fig:error_hist_grid}
\end{figure}

Figure~\ref{fig:error_hist_grid} provides a deeper robustness analysis, showing that while GNN performance degrades significantly in unseen scenarios, as seen in Figure~\ref{fig:error_hist_grid}a, and high-density regimes shown in Figure~\ref{fig:error_hist_grid}b, DeepONet maintains stable low error rates, confirming its superior generalization capabilities.

\subsection{Real-World Validation}\label{sec:module3}
To validate the proposed approach on a complex, non-linear network, we applied the models to the METR-LA benchmark dataset. Unlike the simulation, this dataset involves a graph of 207 sensors with complex spatial dependencies.

Here, the advantages of advanced architectures became evident. DeepONet achieved State-of-the-Art performance with an $R^2$ of 0.9172, significantly outperforming the MLP baseline with an $R^2$ of 0.8791 and surpassing the standard GNN baseline which reached 0.8952. The Transformer also performed exceptionally well at 0.9137. DeepONet's superior performance suggests it can capture propagation effects effectively even without explicit graph convolution layers, likely by learning the high-dimensional mapping of the system's state.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig1_parity_comparison.png}
\caption{Parity plots comparing predicted vs. actual traffic speeds for (a) MLP, (b) GNN, and (c) DeepONet on the Real-World METR-LA dataset. The red dashed line represents perfect prediction ($y=x$). DeepONet shows significantly tighter clustering around the diagonal compared to MLP and GNN, particularly in the high-speed free-flow regime exceeding 40 km/h, demonstrating its superior capability in handling complex real-world dynamics.}
\label{fig:parity_grid}
\end{figure}

Figure~\ref{fig:parity_grid} visualizes this performance gap through parity plots, where DeepONet shows significantly tighter clustering around the diagonal compared to MLP and GNN, particularly in the high-speed free-flow regime.

This result confirms that while simple temporal models suffice for linear simulations, DeepONet and Transformer architectures are essential for capturing the complex, high-dimensional spatiotemporal dynamics of real-world traffic networks. The significant performance gap between DeepONet/Transformer and MLP on real data of approximately 4\% in $R^2$ strongly supports the adoption of operator learning frameworks for practical ITS applications.

It is worth noting that the training times for MLP and LSTM in this module of approximately 4 to 6 seconds are significantly shorter than in the simulation experiments. This is attributed to the smaller dataset size, 34k samples compared to 1.2 million, and the rapid convergence of these baselines, which triggered early stopping around epoch 15. Additionally, the LSTM implementation utilized a vectorized input structure to maximize GPU parallelism, avoiding the high computational cost of sequential unrolling.

The contrast in GNN performance between the simulation in Module 2 and real-world in Module 3 experiments is particularly illuminating. In the sparse, linear simulation topology, GNNs struggled with an $R^2$ of approximately 0.72 as the graph structure provided limited connectivity for effective message passing. However, in the dense, interconnected METR-LA graph, GNNs thrived achieving an $R^2$ of 0.8952, validating their design for graph-structured data. Crucially, DeepONet performed consistently well across both regimes, demonstrating a versatility that neither pure temporal models such as LSTM nor pure spatial models such as GNN could match individually. Figure~\ref{fig:metr_la_forecast} further illustrates this by comparing the time-series forecasts, where DeepONet and Transformer accurately track abrupt speed drops during rush hours, unlike the lagging baselines.

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/fig3_metr_la_forecast.png}
\caption{Time-series forecast comparison on the METR-LA dataset (Node 112). The DeepONet shown in blue and Transformer in green accurately track the abrupt speed drops during morning and evening rush hours, whereas the MLP in orange and GNN in purple exhibit significant lag and fail to capture the full depth of the congestion valleys.}
\label{fig:metr_la_forecast}
\end{figure}

\begin{figure}[H]
\centering
\subfloat[Drop-one trunk feature importance (\(\Delta R^2\) vs.\ DeepONet \(p{=}256\)).]{
  \includegraphics[width=0.49\linewidth]{figures/ablation_dropone_dr2.png}}
\hfill
\subfloat[Capacity sweep for DeepONet (\(p\in\{64,128,256\}\)).]{
  \includegraphics[width=0.43\linewidth]{figures/ablation_capacity_sweep.png}}
\caption{Ablation diagnostics. Removing \emph{density} or \emph{traveltime} causes the largest degradation, confirming the value of exogenous context. Increasing latent width \(p\) steadily improves performance without brittleness.}
\label{fig:ablation}
\end{figure}

\begin{table*}[t]
\centering
\caption{Ablations on input configuration and architecture (held-out scenes).}
\label{tab:ablation}
\small
\setlength{\tabcolsep}{6pt}\renewcommand{\arraystretch}{1.05}
\newcolumntype{C}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{lCCCCCC}
\toprule
\textbf{Configuration} & \textbf{MAE} & \boldmath{$\Delta$} & \textbf{RMSE} & \boldmath{$\Delta$} & \boldmath{$R^{2}$} & \boldmath{$\Delta$} \\
\midrule
DeepONet (Branch-only; 12 lags) & 12.779 & +11.536 & 12.959 & +10.927 & -7.4611 & -8.4547 \\
DeepONet - occupancy            & 1.393  & +0.150  & 2.447  & +0.415  & 0.9828  & -0.0108 \\
DeepONet - density              & 20.193 & +18.950 & 38.480 & +36.448 & -3.2520 & -4.2456 \\
DeepONet - traveltime           & 3.065  & +1.822  & 3.157  & +1.125  & 0.4979  & -0.4957 \\
DeepONet - entered              & 3.782  & +2.539  & 4.974  & +2.942  & 0.9289  & -0.0647 \\
DeepONet - left                 & 3.112  & +1.869  & 4.423  & +2.391  & 0.9438  & -0.0498 \\
DeepONet - waitingTime          & 1.884  & +0.641  & 2.671  & +0.639  & 0.9795  & -0.0141 \\
DeepONet (p=64)                 & 1.310  & +0.067  & 2.140  & +0.108  & 0.9478  & -0.0458 \\
DeepONet (p=128)                & 1.392  & +0.149  & 2.127  & +0.095  & 0.9484  & -0.0452 \\
\textbf{DeepONet (p=256)}       & \textbf{1.243} & +0.000 & \textbf{2.032} & +0.000 & \textbf{0.9936} & +0.0000 \\
Concat-MLP (18-d)               & 1.430  & +0.187  & 2.243  & +0.211  & 0.9856  & -0.0080 \\
\bottomrule
\end{tabularx}
\end{table*}

\subsection{Ablation Study}
To verify the contribution of each component in the DeepONet architecture, we conducted an ablation study by varying the network structure and latent dimension $p$. Table~\ref{tab:ablation} summarizes the ablation results tested on the unfiltered simulation data, which contains a significant number of zero values compared to the filtered dataset used in the main experiments. As shown in Figure~\ref{fig:ablation}, removing the Branch network, which relies solely on the Trunk network for exogenous features, leads to a significant performance drop of approximately 15\%, confirming that the historical state trajectory encoded by the Branch network is critical for accurate forecasting. Furthermore, we analyzed the sensitivity to the latent dimension $p$. Performance degrades noticeably when $p < 32$, indicating underfitting, while increasing $p$ beyond 128 yields diminishing returns, justifying our choice of $p=128$ as an optimal balance between accuracy and computational efficiency.

In addition to architectural components, we evaluated the impact of specific trunk features. Our analysis identified density and travel time as the most critical exogenous variables.

\subsection{Discussion}
The experimental results highlight several key characteristics of the DeepONet framework for traffic forecasting. First, the model demonstrates remarkable robustness across varying topological complexities. In the linear SUMO simulation, it performs on par with specialized sequence models like LSTM, while in the complex METR-LA network, it achieves state-of-the-art performance comparable to Transformers and superior to standard GNNs. This suggests that the operator learning paradigm, which maps functional spaces rather than discrete points, effectively captures the underlying physical dynamics of traffic flow regardless of the specific network structure.

Second, the "Digital Twin" capability, evidenced by the recovery of the fundamental diagram in Figure~\ref{fig:counterfactual}, distinguishes DeepONet from purely statistical baselines. By learning the operator $G: u \to G(u)$, the model does not merely memorize historical patterns but internalizes the causal relationship between density and speed. This allows for reliable counterfactual reasoning, a critical feature for logistics planning where operators must evaluate hypothetical scenarios that may differ from historical averages.

To further investigate the model's sensitivity to specific boundary conditions, we performed a systematic perturbation analysis.
\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{figures/counterfactual_grid.png}
\caption{Zero-retraining counterfactual responses to multiplicative perturbations of trunk features. For each feature $x_j \in \{\text{occupancy, density, traveltime, entered, left, waitingTime}\}$, we evaluate the mean predicted speed after scaling that feature by $(1 + \epsilon)$ with $\epsilon \in \{-0.10, -0.05, 0, 0.05, 0.10\}$, holding all other inputs fixed. DeepONet (blue) and a concatenation MLP (orange) are compared.}
\label{fig:perturbation_grid}
\end{figure}
Figure~\ref{fig:perturbation_grid} shows the mean predicted speed response to multiplicative scaling of each trunk feature. DeepONet exhibits physically consistent sensitivity, particularly to density and occupancy, whereas the MLP baseline often shows negligible or erratic responses, confirming the operator model's superior ability to disentangle causal factors.

Third, our analysis sheds light on the nature of the traffic modeling challenge. Given that traffic variables such as density, speed, and travel time are highly correlated, we investigated potential multicollinearity issues by comparing DeepONet with Ridge regression, which is robust to multicollinearity via L2 regularization. Ridge regression performed poorly on the simulation dataset ($R^2 \approx 0.46$) but achieved high accuracy on the METR-LA dataset ($R^2 \approx 0.90$). This stark contrast indicates that the primary challenge in the simulation environment is non-linearity, specifically the regime shifts between free-flow and congestion, rather than multicollinearity. The superior performance of DeepONet stems from its ability to model these non-linear operator mappings, which linear models like Ridge cannot capture effectively, regardless of their robustness to collinearity.

However, certain limitations warrant discussion. While DeepONet outperforms MLP and Ridge regression, its training time is higher, though still competitive with LSTM. Conversely, in terms of inference efficiency, DeepONet demonstrates a clear advantage. As shown in Table~\ref{tab:overall_comparison}, its inference time (0.07s) is significantly lower than that of the Transformer (0.33s) and GNN (0.18s), making it highly suitable for real-time applications where low latency is critical. Additionally, unlike GNNs which explicitly encode the adjacency matrix, DeepONet learns spatial dependencies implicitly through the Trunk network's conditioning. While this proved effective in our experiments, it may face scalability challenges in extremely large networks where the explicit sparsity of graph convolutions offers a computational advantage. Nevertheless, the results confirm that for typical urban traffic networks, DeepONet provides a versatile and powerful alternative to existing spatiotemporal architectures.

\section{Conclusion}
This study presented a Deep Operator Network framework for macroscopic speed forecasting that explicitly links logistics demand to traffic states. By factorizing the learning problem into a branch network for historical dynamics and a trunk network for exogenous boundary conditions, the model achieves robust cross-scenario generalization without retraining, effectively addressing the challenges of distribution shift and data heterogeneity. For logistics operators, this capability enables 'what-if' analysis of routing strategies under varying congestion regimes. For traffic managers, it provides a data-driven digital twin that adapts to shifting demand patterns. Figure~\ref{fig:counterfactual} demonstrates this "Digital Twin" capability, where the model correctly recovers the fundamental diagram of traffic flow from hypothetical inputs, validating its physical consistency.

Despite these promising results, the current model relies on aggregated link-level features and does not explicitly capture network topology via graph convolutions, which may limit performance in large-scale networks with complex propagation effects. Additionally, while we validated the model on both SUMO simulation and METR-LA real-world data, the transfer learning capability between simulation and reality (Sim2Real) remains to be fully explored. Future research will focus on integrating Graph Neural Operators to better capture spatial dependencies, incorporating physical constraints via Physics-Informed Neural Networks (PINNs) to enhance interpretability, and developing unsupervised domain adaptation techniques to further bridge the gap between logistics planning simulations and real-time traffic operations. Together, these directions chart a pathway from accurate one-step forecasts to robust, decision-ready tools for real-world logistics networks.


\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{figures/fig5_counterfactual.png}
\caption{Counterfactual analysis demonstrating the "Digital Twin" capability. We queried the trained DeepONet with hypothetical input functions representing increasing traffic density. The model correctly recovered the fundamental diagram of traffic flow (Speed vs. Density inverse relationship) without ever being explicitly trained on physics equations, validating its ability to learn the underlying operator.}
\label{fig:counterfactual}
\end{figure}

\section*{Author Contributions}
Conceptualization, Bin Yu; methodology, Dawei Luo and Yong Chen; software, Bin Yu and Dawei Luo; validation, Bin Yu; formal analysis, Bin Yu; investigation, Bin Yu and Dawei Luo; resources, Bin Yu; data curation, Bin Yu; writing—original draft, Bin Yu; writing—review \& editing, Joonsoo Bae; visualization, Bin Yu; supervision, Bin Yu; project administration, Bin Yu. All authors have read and agreed to the published version of the manuscript.

\section*{Funding}
This work was supported by the China Society of Logistics (CSL) Research Projects:
(1) “Path Identification and Strategy for Digital Transformation in Small and Medium-sized Logistics Enterprises” (Grant No. 2025CSLKT3-083, 2025);
(2) “Operation Workflow of Smart Factory Production Logistics and AGV Path Optimization” (Grant No. 2024CSLKT3-089, 2024).


\section*{Data Availability Statement}
Simulation scripts and training code are available at \cite{scm_deeponet_repo}.

\section*{Conflicts of Interest}
The authors declare no conflict of interest.



\acknowledgments{In this section you can acknowledge any support given which is not covered by the author contribution or funding sections. This may include administrative and technical support, or donations in kind (e.g., materials used for experiments). Where GenAI has been used for purposes such as generating text, data, or graphics, or for study design, data collection, analysis, or interpretation of data, please add “During the preparation of this manuscript/study, the author(s) used [tool name, version information] for the purposes of [description of use]. The authors have reviewed and edited the output and take full responsibility for the content of this publication.”}

\conflictsofinterest{Declare conflicts of interest or state ``The authors declare no conflicts of interest.'' Authors must identify and declare any personal circumstances or interest that may be perceived as inappropriately influencing the representation or interpretation of reported research results. Any role of the funders in the design of the study; in the collection, analyses or interpretation of data; in the writing of the manuscript; or in the decision to publish the results must be declared in this section. If there is no role, please state ``The funders had no role in the design of the study; in the collection, analyses, or interpretation of data; in the writing of the manuscript; or in the decision to publish the results''.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional

%% Only for journal Encyclopedia
%\entrylink{The Link to this entry published on the encyclopedia platform.}

\abbreviations{Abbreviations}{
The following abbreviations are used in this manuscript:
\\

\noindent 
\begin{tabular}{@{}ll}
Adam & Adaptive Moment Estimation \\
ARIMA & AutoRegressive Integrated Moving Average \\
DeepONet & Deep Operator Network \\
FFT & Fast Fourier Transform \\
GNN & Graph Neural Network \\
GPS & Global Positioning System \\
ITS & Intelligent Transportation Systems \\
LSTM & Long Short-Term Memory \\
MAE & Mean Absolute Error \\
MLP & Multilayer Perceptron \\
OD & Origin--Destination \\
PDE & Partial Differential Equation \\
ReLU & Rectified Linear Unit \\
RMSE & Root Mean Squared Error \\
SUMO & Simulation of Urban MObility \\
TCN & Temporal Convolutional Network \\
$R^2$ & Coefficient of Determination
\end{tabular}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Optional
\appendixtitles{no} % Leave argument "no" if all appendix headings stay EMPTY (then no dot is printed after "Appendix A"). If the appendix sections contain a heading then change the argument to "yes".
\appendixstart
\appendix
\section[\appendixname~\thesection]{}
\subsection[\appendixname~\thesubsection]{}
The appendix is an optional section that can contain details and data supplemental to the main text---for example, explanations of experimental details that would disrupt the flow of the main text but nonetheless remain crucial to understanding and reproducing the research shown; figures of replicates for experiments of which representative data are shown in the main text can be added here if brief, or as Supplementary Data. Mathematical proofs of results not central to the paper can be added as an appendix.

\begin{table}[H] 
\caption{This is a table caption.\label{tab5}}
%\newcolumntype{C}{>{\centering\arraybackslash}X}
\begin{tabularx}{\textwidth}{CCC}
\toprule
\textbf{Title 1}	& \textbf{Title 2}	& \textbf{Title 3}\\
\midrule
Entry 1		& Data			& Data\\
Entry 2		& Data			& Data\\
\bottomrule
\end{tabularx}
\end{table}

\section[\appendixname~\thesection]{}
All appendix sections must be cited in the main text. In the appendices, Figures, Tables, etc. should be labeled, starting with ``A''---e.g., Figure A1, Figure A2, etc.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\isPreprints{}{% This command is only used for ``preprints''.
\begin{adjustwidth}{-\extralength}{0cm}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
%\printendnotes[custom] % Un-comment to print a list of endnotes

\reftitle{References}

% Please provide either the correct journal abbreviation (e.g. according to the “List of Title Word Abbreviations” http://www.issn.org/services/online-services/access-to-the-ltwa/) or the full name of the journal.
% Citations and References in Supplementary files are permitted provided that they also appear in the reference list here. 

%=====================================
% References, variant A: external bibliography
\bibliography{refs}
%=====================================


%=====================================
% References, variant B: internal bibliography
%=====================================



% If authors have biography, please use the format below
%\section*{Short Biography of Authors}
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author1.pdf}}}
%{\textbf{Firstname Lastname} Biography of first author}
%
%\bio
%{\raisebox{-0.35cm}{\includegraphics[width=3.5cm,height=5.3cm,clip,keepaspectratio]{Definitions/author2.jpg}}}
%{\textbf{Firstname Lastname} Biography of second author}

% For the MDPI journals use author-date citation, please follow the formatting guidelines on http://www.mdpi.com/authors/references
% To cite two works by the same author: \citeauthor{ref-journal-1a} (\citeyear{ref-journal-1a}, \citeyear{ref-journal-1b}). This produces: Whittaker (1967, 1975)
% To cite two works by the same author with specific pages: \citeauthor{ref-journal-3a} (\citeyear{ref-journal-3a}, p. 328; \citeyear{ref-journal-3b}, p.475). This produces: Wong (1999, p. 328; 2000, p. 475)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% for journal Sci
%\reviewreports{\\
%Reviewer 1 comments and authors’ response\\
%Reviewer 2 comments and authors’ response\\
%Reviewer 3 comments and authors’ response
%}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\PublishersNote{}
%\isPreprints{}{% This command is only used for ``preprints''.
\end{adjustwidth}
%} % If the paper is ``preprints'', please uncomment this parenthesis.
\end{document}

